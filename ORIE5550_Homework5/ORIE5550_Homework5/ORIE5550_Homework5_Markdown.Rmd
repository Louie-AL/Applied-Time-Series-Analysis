---
title: "ORIE5550_Homework5_Markdown"
author: "Luis Alonso Cendra Villalobos (lc2234)"
date: "2024-03-07"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
---

```{r setup, include=FALSE}
#install.packages("astsa")
#install.packages("perARMA")
library(perARMA)
library(astsa)
library(forecast)
library(urca)

#load("h5q4_student.RData")

```

# Problem 1

## Consider the following seasonal MA model with zero mean and period $s = 12$  given by $X_t = (I+\theta_1B)(I+\Theta_1B^{12})Z_t = (I+\theta_1B + \Theta_1B^{12} + \theta_1 \Theta_1 B^{13})Z_t$ with $Z_t \sim WN(0, \sigma_Z^2)$. This model is denoted SARIMA(0,0,1) × (0,0,1)[12]. Do the following for this model.

### (a) Compute theoretically the ACF for this model; Then, fix $\theta_1 = 0.7$ and $\Theta_1 = −0.5$. Use ARMAacf to check if your theoretical ACF is identical to the result from the function for lags $h =1,...,30$

```{r q1a}

#theoretical ACF calculation
custom_function <- function(x) {
  if (x == 0) {
    return(1)
  } else if (x == 1) {
    return(0.4698)
  } else if (x == 11 | x == 13) {
    return(-0.1879)
  } else if (x == 12) {
    return(-0.4)
  } else {
    return(0)
  }
}
x_values <- 0:30
function_values <- sapply(x_values, custom_function)

# ACF and PACF generated by R's ARMAacf
ma_acf <- ARMAacf(ma = c(0.7,0,0,0,0,0,0,0,0,0,0,-0.5,-0.5*0.7),lag.max=30,pacf=F)
ma_pacf <- ARMAacf(ma = c(0.7,0,0,0,0,0,0,0,0,0,0,-0.5,-0.5*0.7),lag.max=30,pacf=T)

par(mfrow = c(1, 3))
plot(ma_acf, type='h', main = "ARMAACF")
plot(x_values, function_values, main = "Theoretical ACF")
plot(ma_pacf, type='h', main = "PACF")

```


### (b) Use set.seed($99$) to generate time series data of length $300$ from this model assuming that $\theta_1 = 0.7$, $\Theta_1 = −0.5$, and {Zt} is IID standard normal; Produce a time plot and sample ACF and PACF plots

```{r q1b}

# SARIMA$(1,0,0)\times (1,0,0)_{12}$
set.seed(99)
ts.e1 <- sarima.sim(ma=0.7, sma= -0.5, S=12,n=300)
par(mfrow = c(1, 3))
plot.ts(ts.e1)
acf(ts.e1,lag.max = 30, main= "ACF")
pacf(ts.e1,lag.max = 30, main= "PACF")

```

### (c) For the generated data in (b), fit a SARIMA(0,0,1)×(0,0,1)12 model by using the function Arima from the R package forecast; Write down the exact model that was fitted.

```{r q1c}

sarima_fit <- arima(ts.e1, order = c(0, 0, 1), 
                    seasonal = list(order = c(0, 0, 1), period = 12),
                    include.mean = FALSE)
sarima_fit

theta_ma1 = sarima_fit$coef[1]
theta_sma1 = sarima_fit$coef[2]

```

The exact model is:

\begin{center}
$X_t = Z_{t} + `r round(theta_ma1,2)`Z_{t-1} + `r round(theta_sma1,2)`Z_{t-12} + (`r round(theta_ma1,2)`) * (`r round(theta_sma1,2)`) Z_{t-13} $ with $\{Z_t: t \in \mathbb{Z}\} \sim WN(0, \sigma_z^2)$
\end{center}

### (d) For the generated data, produce 12-steps-ahead forecasts by using the R function forecast and the model fitted in part (c); Include the usual plot with forecasts.

```{r q1d}
h <- 12
ts.e1.forecast <- forecast(sarima_fit, h)
plot(ts.e1.forecast)

```

# Problem 2. 

### Consider the monthly U.S. unemployment series unemp in the R package astsa. Leave out the last 12 observations and denote these samples as test data.

### (a) Take the logarithm of the series remaining unemp and call the result series training data. By referring to Problem 2 in Homework 2, fit a quadratic polynomial trend and a seasonal component to the training data using regression; Consider the residual series from the regression and use the function auto.arima to fit the “best” SARIMA model for it; Use this SARIMA model to forecast the residual series 12 steps ahead; Finally, combined with the regression, translate this forecast into the 12-step-ahead predictions of the “original scale”; On the time plot of unemp (the whole series), overlap original scale of fitted values and predictions with different colors.

```{r q2a}

unemp_data <- ts(unemp, start = c(1948, 1), end = c(1978, 12) , frequency = 12)
plot(unemp_data, type="l", main="Box-Cox Log-Transform of the GNP data",
ylab = "log(gnp)", xlab = "Year")

# Leave out the last 12 observations as test data
training_data <- head(unemp_data,-12)
training_data = log(training_data)
test_data <- tail(unemp_data, 12)
tt <- seq(1, length(training_data), by=1)
tt2 <- tt^2
y <- training_data

fitModel <- lm( y ~ tt + tt2 +
              cos(2*tt*pi/12) + sin(2*tt*pi/12) +
              cos(2*tt*pi/6) + sin(2*tt*pi/6) +
              cos(2*tt*pi/4) + sin(2*tt*pi/4))

summary(fitModel)

residuals <- ts(fitModel$residuals, frequency=12)

auto_sarima <- auto.arima(residuals, max.p = 5, max.q = 5, max.d = 2,
                        start.p = 0, start.q = 0,
                        max.P = 5, max.Q = 5, max.D = 2,
                        start.P = 0, start.Q = 0,
                        allowdrift = FALSE, ic = "aic")

h <- 12
residuals.forecast <- forecast(auto_sarima, h)
summary(residuals.forecast)
plot(residuals.forecast)

forecasted_residuals<-matrix(0,nrow=1,ncol=12)
for (i in 1:12) {
  forecasted_residuals[1,i]<-as.numeric(residuals.forecast$mean)[i]
}

estimated_coeffs<-matrix(0,nrow=1,ncol=9)
for (i in 1:9) {
  estimated_coeffs[1,i]<-fitModel$coefficients[i]
}

time_matrix<-matrix(0,nrow=12,ncol=9)
time_matrix[,1]<- 1
time_matrix[,2]<- 361:372
time_matrix[,3]<- time_matrix[,2]^2
time_matrix[,4]<- cos(2 * time_matrix[,2] * pi / 12)
time_matrix[,5]<- sin(2 * time_matrix[,2] * pi / 12)
time_matrix[,6]<- cos(2 * time_matrix[,2] * pi / 6)
time_matrix[,7]<- sin(2 * time_matrix[,2] * pi / 6)
time_matrix[,8]<- cos(2 * time_matrix[,2] * pi / 4)
time_matrix[,9]<- sin(2 * time_matrix[,2] * pi / 4)
time_matrix<-t(time_matrix)

trendAndseasonal_forecast = estimated_coeffs %*% time_matrix
ts.forecast.pred<-trendAndseasonal_forecast + forecasted_residuals
exp(ts.forecast.pred)
msfe_2a <-mean((exp(ts.forecast.pred)-as.numeric(test_data))^2)
msfe_2a


##############
##Fitted values


estimated_coeffs<-matrix(0,nrow=1,ncol=9)
for (i in 1:9) {
  estimated_coeffs[1,i]<-fitModel$coefficients[i]
}

time_matrix<-matrix(0,nrow=360,ncol=9)
time_matrix[,1]<- 1
time_matrix[,2]<- 1:360
time_matrix[,3]<- time_matrix[,2]^2
time_matrix[,4]<- cos(2 * time_matrix[,2] * pi / 12)
time_matrix[,5]<- sin(2 * time_matrix[,2] * pi / 12)
time_matrix[,6]<- cos(2 * time_matrix[,2] * pi / 6)
time_matrix[,7]<- sin(2 * time_matrix[,2] * pi / 6)
time_matrix[,8]<- cos(2 * time_matrix[,2] * pi / 4)
time_matrix[,9]<- sin(2 * time_matrix[,2] * pi / 4)
time_matrix<-t(time_matrix)

fitted_values = estimated_coeffs %*% time_matrix


#Plotting

library(ggplot2)
library(tidyr)

df1 <- data.frame(train_compare = exp(training_data), timestamp = seq(1,360,1))
df2 <- data.frame(forecast_data = as.numeric(exp(ts.forecast.pred)), timestamp = seq(361,372,1))
df3 <- data.frame(fitted_compare = as.numeric(exp(estimated_coeffs %*% time_matrix)), timestamp = seq(1,360,1))
merged_df <- merge(df1, df2, by = "timestamp", all = TRUE)
merged_df <- merge(merged_df, df3, by = "timestamp", all = TRUE)

ggplot(merged_df, aes(x = timestamp)) +
  geom_line(aes(y = train_compare, color = "Training Data")) +
  geom_line(aes(y = forecast_data, color = "Forecast Data")) +
  geom_line(aes(y = fitted_compare, color = "Fitted Values")) +
  labs(x = "Timestamp", y = "Value", color = "Data Type") +
  ggtitle("Original, Fitted and Forecasted TS") +
  theme_minimal()

```

The original scale forecasts are:  `r (exp(ts.forecast.pred))`

### (b) For the same training data, fit the “best” SARIMA model (allowing for the differencing and seasonal differencing); Use this SARIMA model to forecast 12 steps ahead; Translate this forecast into the 12-step-ahead predictions of the “original scale”; On the time plot of unemp, overlap original scale of fitted values and predictions with different colors.

```{r q2b}
sarima_fit_2b <- auto.arima(training_data, max.p = 5, max.q = 5, max.d = 2,
                        start.p = 0, start.q = 0,
                        max.P = 5, max.Q = 5, max.D = 2,
                        start.P = 0, start.Q = 0,
                        allowdrift = FALSE, ic = "aic")

summary(sarima_fit_2b)

h <- 12
unemp.forecast <- forecast(sarima_fit_2b, h)
unemp.forecast$mean
original_series_forecast = exp(unemp.forecast$mean)
msfe_2b<-mean((original_series_forecast - as.numeric(test_data))^2)
msfe_2b


#Plotting

library(ggplot2)
library(tidyr)

df1 <- data.frame(train_compare = exp(training_data), timestamp = seq(1,360,1))
df2 <- data.frame(forecast_data = original_series_forecast, timestamp = seq(361,372,1))
df3 <- data.frame(fitted_compare = exp(sarima_fit_2b$fitted), timestamp = seq(1,360,1))
merged_df <- merge(df1, df2, by = "timestamp", all = TRUE)
merged_df <- merge(merged_df, df3, by = "timestamp", all = TRUE)

ggplot(merged_df, aes(x = timestamp)) +
  geom_line(aes(y = train_compare, color = "Training Data")) +
  geom_line(aes(y = forecast_data, color = "Forecast Data")) +
  geom_line(aes(y = fitted_compare, color = "Fitted Values")) +
  labs(x = "Timestamp", y = "Value", color = "Data Type") +
  ggtitle("Original, Fitted and Forecasted TS") +
  theme_minimal()

```

###  (c) Compare the predicted values in (a) and (b) with test data by computing the mean squared prediction errors; Which of the approaches, (a) or (b), gives a smaller prediction error?

Approach (b) has a lower mean squared forecast error of `r (msfe_2b)` than approach (a) with a MSFE of `r (msfe_2a)`. We can see from the fitted values compassion to the original series that the approach (a) fits the trend reasonably but misses some of the seasonality and spikes from the unemployment data, which are better approximated by the SARIMA model.
 

# Problem 3

### (a) Write down the exact form of the PAR(2) model that the series is generated from. That is, specify the parameters $\phi_{j,k},$\sigma_{Z,k}^2 and $m_k$ for $j = 1,2$ and $k = 1,...,24$

```{r q3a}

set.seed(1)
s <- 24
TT <- 1000
p <- 2
a <- matrix(0,s,p)
a[1,1] <- 0.5
a[2,2] <- 0.4
phia <- ab2phth(a)
phi0 <- phia$phi
phi0 <- as.matrix(phi0)
del0 <- matrix(1,s,1)
PAR2 <- makepar(TT,phi0,del0)
par2 <- PAR2$y
plot(ts(par2))

```

### (b) Produce the sample ACF and PACF plots of the generated series. Treating the series as zero mean stationary, which zero mean stationary AR model is suggested by the auto.arima function?

```{r q3b}

par(mfrow=c(1,2))
acf(par2,lag.max=20)
pacf(par2,lag.max=20)

AR_model <- auto.arima(par2, max.p = 5, max.q = 5, max.d = 2,
                        start.p = 0, start.q = 0,
                        max.P = 5, max.Q = 5, max.D = 2,
                        start.P = 0, start.Q = 0,
                        allowmean = FALSE, stationary = "TRUE",
                        ic = "aic")

summary(AR_model)
phi_1 = AR_model$coef[1]



```

The model suggested is an AR(1):

\begin{center}
$X_t = `r round(phi_1,4)`X_{t-1} + Z_{t} $ with $\{Z_t: t \in \mathbb{Z}\} \sim WN(0, \sigma_z^2)$
\end{center}

### (c) Use the command perYW(par,24,2,NaN) to fit the PAR(2) model for the generated series; Produce two plots: One plot showing $\hat{\phi_{1,k}}$ and $\phi_{1,k}$ for $k = 1,...,24$ and the other showing $\hat{\phi_{2,k}}$ and $\phi_{2,k}$ 


```{r q3c}

out.par<-perARMA::perYW(par2,24,2,NaN)

plot.new() 

plot(out.par$phi[,1], type = 'b', col = 'blue', xlab = 'k', ylab = 'Value', main = expression(hat(phi[1,k]) * " and " * phi[1,k] * " for k = 1,...,24"))
lines(phi0[,1], col = 'red', type = 'b')

plot(out.par$phi[,2], type = 'b', col = 'blue', xlab = 'k', ylab = 'Value', main = expression(hat(phi[2,k]) * " and " * phi[2,k] * " for k = 1,...,24"))
lines(phi0[,2], col = 'red', type = 'b')

```

# Problem 4.
Consider bank calls from the R package fpp3 in Problem 4 (b) in Homework 2. We will use the aggregated time series as in the previous homework. The code hw5student.R for producing the series can be found in Homework folder on Canvas. Do the following.

```{r q4}

rm(list=ls())
library(fpp3)
library(perARMA)
library(partsm)

data(bank_calls)

# bank_calls$DateTime[1:24]
# bank_calls$DateTime[((7-1)*24+1):(7*24)]
# bank_calls$DateTime[c(7*24+1,7*24+2,7*24+3)]

TT <- length(bank_calls$Calls)
floor(TT/169)
floor(TT/169)*169 - TT

ts <- vector("numeric",169)
for (k in 1:169){
  
  if (k %% 7 == 0){
    ts[k] <- sum(bank_calls$Calls[((k-1)*24+1):(k*24+1)])
  }else{
    ts[k] <- sum(bank_calls$Calls[((k-1)*24+1):(k*24)])
  }
}

# 168 = 24*7
ts168 <- log(ts[1:168])
ts168 <- ts(ts168,start=c(1,1),end=c(24,7),frequency=7)
plot.ts(ts168)

# Hold out the last 7 observations
ts168_train <- ts(ts168[-c(162:168)],start=c(1,1),end=c(23,7),frequency=7)
ts168_test <- ts(ts168[c(162:168)],start=c(24,1),end=c(24,7),frequency=7)

plot.ts(ts168)
lines(ts168_train,col="red")
lines(ts168_test,col="blue")

ts168_test_demean <- ts168_test - mean(ts168_test)

```

### (a) For ts168 train, calculate and remove weekly means; Then calculate and remove seasonal/periodic means; Denote the series that the weekly means and seasonal/periodic means are removed by ts168_train3. Produce a time plot and sample ACF and PACF plots of the series.

```{r q4a}
library(fpp3)

#Weekly means
weekly_mean<-c(mean(ts168_train[1:35]),diff(cumsum(ts168_train)[seq(35,140,35)]/35),mean(ts168_train[141:161]))

#Demean the series
ts168_train2 <- ts168_train - head(rep(weekly_mean,each=35),161)

#calculating and removing seasonal/periodic means
ts168_train<-as.numeric(ts168_train)
ts168_train_perm <-permest(t(ts168_train),T_t=7,alpha=0.05,missval=NaN,datastr='ts168_train')
ts168_train3<-ts168_train-rep(ts168_train_perm$pmean,23) #161/7 = 23


par(mfrow=c(1,3))
plot.ts(ts168_train3)
acf(ts168_train3,lag.max=50)
pacf(ts168_train3,lag.max=50)

```

### (b) For ts168_train3. fit PAR(p) with season s = 7 by using fit.ar.par. The lag of the AR coefficients can be chosen by aic or bic; Also fit SARMA(p,0)×(P,0)[7] by using auto.arima with proper input;

```{r q4b}
library(partsm)

detcomp<-list(regular=c(0,0,0),seasonal=c(1,0),regvar=0)

#lag section:
aic<-bic<-Fnextp<-Fpval<-rep(NA,10)
for(p in 1:10){
  
 lmpar<-fit.ar.par(wts=ts168_train3,type="PAR",detcomp=detcomp,p=p)
 aic[p]<-AIC(lmpar@lm.par,k=2)
 bic[p]<-BIC(lmpar@lm.par)
 
 #H_0:phi_{p+1,k}=0,k=1,...,s
 Fout<-Fnextp.test(wts=ts168_train3,detcomp=detcomp,p=p,type="PAR")
 Fnextp[p] <-Fout@Fstat
 Fpval[p]<-Fout@pval
}
which.min(aic)
which.min(bic)

p_par <- 8 # selected lag order
out.par<-fit.ar.par(wts=ts168_train3,type="PAR",detcomp=detcomp,p=p_par)
summary(out.par)

# SARMA Model
# seasonality 7
ts168_train3 <- ts(ts168_train3, frequency=7)
# only allow p and P lags
SARMA_q4 <- auto.arima(ts168_train3, max.p = 10, max.q = 0, max.d = 0,
                        start.p = 0, start.q = 0,
                        max.P = 10, max.Q = 0, max.D = 0,
                        start.P = 0, start.Q = 0,
                        allowdrift = FALSE, ic = "aic")
summary(SARMA_q4)

```

### (c) Draw a time series plot of ts168 train3 and overlap the fitted values of PAR and SAR models with different colors; Compute the mean squared error of the residuals from both models.

```{r q4c}
par.mod<-slot(out.par,"lm.par")

df1 <- data.frame(train_compare = ts168_train3, timestamp = seq(1,161,1))
df2 <- data.frame(par_fitted = par.mod$fitted.values, timestamp = seq(9,161,1))
df3 <- data.frame(sarma_fitted = SARMA_q4$fitted, timestamp = seq(1,161,1))
merged_df <- merge(df1, df2, by = "timestamp", all = TRUE)
merged_df <- merge(merged_df, df3, by = "timestamp", all = TRUE)

ggplot(merged_df, aes(x = timestamp)) +
  geom_line(aes(y = train_compare, color = "Training Data")) +
  geom_line(aes(y = par_fitted, color = "Par Fit")) +
  geom_line(aes(y = sarma_fitted, color = "SARMA Fit")) +
  labs(x = "Timestamp", y = "Value", color = "Data Type") +
  ggtitle("Original, Fitted and Forecasted TS") +
  theme_minimal()


# Mean squared Residuals

residuals_par <- par.mod$residuals
residuals_mse_par <- mean((residuals_par)^2)
residuals_mse_par

residuals_sarma <- SARMA_q4$residuals
residuals_mse_sarma <- mean((residuals_sarma)^2)
residuals_mse_sarma
```

### (d) Produce 7-step-ahead forecasts by using predictperYW for the PAR model and forecast for the SAR model; For the series ts168 test demean given in the code h5p4student.R, compute mean squared forecast errors with seasonal/periodic mean + forecast from the PAR  model and seasonal/periodic mean + SAR model; Conclude which time series model, SAR or PAR, has the smaller forecast error.

```{r q4d}
par.pred<-predictperYW(ts168_train_perm$xd,7,1,NaN,(161+7))$new

h <- 7
SARMA_q4.forecast <- forecast(SARMA_q4, h)
plot(SARMA_q4.forecast)

#par_demean_forecast = (ts168_train_perm$pmean + as.numeric(par.pred))
MSFE_par_demean = mean((as.numeric(par.pred) - ts168_test_demean)^2) 
MSFE_par_demean

#sarma_demean_forecast = (ts168_train_perm$pmean + as.numeric(SARMA_q4.forecast$mean))
MSFE_sarma_demean = mean((as.numeric(SARMA_q4.forecast$mean) - ts168_test_demean)^2) 
MSFE_sarma_demean
```


### Conclusion:
PAR model lower mean squared forecast error of `r (MSFE_par_demean)` than the SARMA model with a MSFE of `r (MSFE_sarma_demean)`. In question 4.c., we saw how the mean squared residuals of the PAR model are lower `r (residuals_mse_par)` than those of the SARMA model `r (residuals_mse_sarma)`. In the plot, we can observe how both models track the series well, but the PAR model out-performs SARMA in periods of high volatility (spikes). 
